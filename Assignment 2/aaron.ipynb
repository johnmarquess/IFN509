{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SANDBOX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project A\n",
    "\n",
    "D1.csv  \n",
    "Knowledge: What categories of products are bought together instead of individually  \n",
    "\n",
    "\n",
    "- Prepare a transactional dataset where each transaction represents the category of products along with other details.  \n",
    "- Build an association mining model on this dataset to identify the common product categories that customers purchase.\n",
    "\n",
    "\n",
    "1. What pre-processing was required on the dataset before building the association mining model? What variables did you include in the analysis? Justify your choice.\n",
    "\n",
    "\n",
    "2. Conduct association mining and answer the following:\n",
    "- What ‘min_support’ and `min_confidence’ thresholds were set for this mining exercise? Rationalise why these values were chosen.\n",
    "- Report and interpret the top-5 rules (as per Lift values).\n",
    "\n",
    "3. List the five most common product categories that customers bought with the product category ‘01F’.\n",
    "4. Can you perform sequence analysis on this dataset? If yes, present your results. If not, rationalise why.\n",
    "5. How can the outcome of this study be used by the relevant decision-makers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the data file\n",
    "filename1 = 'data/D1.csv'\n",
    "filename2 = 'data/D2.csv'\n",
    "df1 = pd.read_csv(filename1)\n",
    "df2 = pd.read_csv(filename2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 131706 entries, 0 to 131705\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Date          131706 non-null  object \n",
      " 1   Customer_ID   131706 non-null  int64  \n",
      " 2   Sales_ID      131706 non-null  int64  \n",
      " 3   SKU_Category  131706 non-null  object \n",
      " 4   SKU           131706 non-null  object \n",
      " 5   Quantity      131706 non-null  float64\n",
      " 6   Sales_Amount  131706 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 131706 entries and non-null counts shows that there is no missing data. To confirm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date            0\n",
       "Customer_ID     0\n",
       "Sales_ID        0\n",
       "SKU_Category    0\n",
       "SKU             0\n",
       "Quantity        0\n",
       "Sales_Amount    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if we have any null value\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for duplicate rows\n",
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Customer_ID', 'Sales_ID', 'SKU_Category', 'SKU', 'Quantity',\n",
       "       'Sales_Amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(10)\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the knowlege is \"What categories of products are bought together instead of individually\", hence we need to focus on the columns that are giving us these information. The rest of them are not relevant.  \n",
    "\n",
    "\n",
    "The relevant columns are:\n",
    "- Sales_ID: One purchare holds many SKU\n",
    "- SKU_Category: As the knowledge mentioned \"What categories of products are bought together\". Hence our focus would be sku_category rather than sku\n",
    "- Quantity: Task 3 requires to list 5 most common product category customers bought with the category 01F\n",
    "- Sales_Amount: this will be useful for decision making on task 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales_id</th>\n",
       "      <th>sku_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/01/2016</td>\n",
       "      <td>1</td>\n",
       "      <td>X52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/01/2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2/01/2016</td>\n",
       "      <td>3</td>\n",
       "      <td>0H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2/01/2016</td>\n",
       "      <td>4</td>\n",
       "      <td>0H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/01/2016</td>\n",
       "      <td>5</td>\n",
       "      <td>0H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131701</th>\n",
       "      <td>4/07/2016</td>\n",
       "      <td>32900</td>\n",
       "      <td>IEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131702</th>\n",
       "      <td>4/07/2016</td>\n",
       "      <td>32900</td>\n",
       "      <td>N8U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131703</th>\n",
       "      <td>4/07/2016</td>\n",
       "      <td>32900</td>\n",
       "      <td>U5F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131704</th>\n",
       "      <td>4/07/2016</td>\n",
       "      <td>32900</td>\n",
       "      <td>0H2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131705</th>\n",
       "      <td>4/07/2016</td>\n",
       "      <td>32900</td>\n",
       "      <td>Q4N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131706 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  sales_id sku_category\n",
       "0       2/01/2016         1          X52\n",
       "1       2/01/2016         2          2ML\n",
       "2       2/01/2016         3          0H2\n",
       "3       2/01/2016         4          0H2\n",
       "4       2/01/2016         5          0H2\n",
       "...           ...       ...          ...\n",
       "131701  4/07/2016     32900          IEV\n",
       "131702  4/07/2016     32900          N8U\n",
       "131703  4/07/2016     32900          U5F\n",
       "131704  4/07/2016     32900          0H2\n",
       "131705  4/07/2016     32900          Q4N\n",
       "\n",
       "[131706 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reducing the number of columns to have the necessary ones\n",
    "df11 = df1[['Date','Sales_ID', 'SKU_Category']]\n",
    "\n",
    "#lowercasing the feature names for ease\n",
    "df11.columns = [col.lower() for col in df11.columns]\n",
    "\n",
    "df11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data to transactional format:  \n",
    "Each sales_id contains many sku_category. Hence we group categories (sku_category) to per transaction (sales_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales_id\n",
       "1             [X52]\n",
       "2             [2ML]\n",
       "3             [0H2]\n",
       "4             [0H2]\n",
       "5             [0H2]\n",
       "            ...    \n",
       "64678         [YMJ]\n",
       "64679    [FEW, H15]\n",
       "64680         [B93]\n",
       "64681         [P42]\n",
       "64682         [XG4]\n",
       "Name: sku_category, Length: 64682, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions = df11.groupby(['sales_id'])['sku_category'].apply(list)\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64682"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model\n",
    "Since we want to check which (at least) two categoris are purchased in one transaction, therefore the chance of each is 50%.  \n",
    "We check how many categories we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 187\n"
     ]
    }
   ],
   "source": [
    "num_sku_category = len(df11['sku_category'].value_counts())\n",
    "print('Number of categories:', num_sku_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probeblity of an sku_category to happen is 1/n. n=187  \n",
    "Our support is between 1 and 0.005 (min support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_support:  0.0053475935828877\n"
     ]
    }
   ],
   "source": [
    "min_support = 1/num_sku_category\n",
    "print('min_support: ', min_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RelationRecord(items=frozenset({'01F'}), support=0.02682353668717727, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'01F'}), confidence=0.02682353668717727, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'0H2'}), support=0.06429918679076095, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'0H2'}), confidence=0.06429918679076095, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'0KX'}), support=0.006802510744874927, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'0KX'}), confidence=0.006802510744874927, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'0WT'}), support=0.010296527627469776, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'0WT'}), confidence=0.010296527627469776, lift=1.0)]),\n",
       " RelationRecord(items=frozenset({'1EO'}), support=0.013697782999907239, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'1EO'}), confidence=0.013697782999907239, lift=1.0)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support=min_support))\n",
    "results[:5]\n",
    "\n",
    "# from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "## Generate frequent itemsets\n",
    "#transaction_list = list(transactions)\n",
    "#frequent_itemsets = apriori(transaction_list, min_support=min_support, use_colnames=True)\n",
    "\n",
    "## Generate association rules with min_confidence\n",
    "#rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "\n",
    "## Display the top 5 rules\n",
    "#print(rules.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rules: 142\n"
     ]
    }
   ],
   "source": [
    "# Out of 64682 transactions 142 transactions with min support of ~0.005\n",
    "print('Number of rules:', len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Left_side Right_side   Support  Confidence      Lift\n",
      "0                    01F  0.026824    0.026824  1.000000\n",
      "1                    0H2  0.064299    0.064299  1.000000\n",
      "2                    0KX  0.006803    0.006803  1.000000\n",
      "3                    0WT  0.010297    0.010297  1.000000\n",
      "4                    1EO  0.013698    0.013698  1.000000\n",
      "..        ...        ...       ...         ...       ...\n",
      "303       N8U    LPF,OXH  0.006973    0.045519  2.268294\n",
      "304       OXH    LPF,N8U  0.006973    0.167161  9.919540\n",
      "305   LPF,N8U        OXH  0.006973    0.413761  9.919540\n",
      "306   LPF,OXH        N8U  0.006973    0.347458  2.268294\n",
      "307   OXH,N8U        LPF  0.006973    0.626389  8.715011\n",
      "\n",
      "[308 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "     \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', \n",
    "                                        'Confidence', 'Lift']) \n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that many items have min support == confidance. These items do not have LHS. This causes the lift to be 1.00, hence these data are not useful and should be removed. However these are still useful to get the frequency of their purchases. For example 0H2 sku_category has a frequency of 0.064 (6.4% times purchased) which is quite high compare to othere categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out those without Left_side\n",
    "def filter_out_lhs(df):\n",
    "    filtered_df = df[(df['Left_side'].notna()) & (df['Left_side'] != '')]\n",
    "    return filtered_df\n",
    "    # filtered_df = df[(df['Left_side'].notna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Left_side Right_side   Support  Confidence      Lift\n",
      "80        01F        6BZ  0.005612    0.209222  7.460249\n",
      "81        6BZ        01F  0.005612    0.200110  7.460249\n",
      "83        01F        FU5  0.007498    0.279539  6.689284\n",
      "84        FU5        01F  0.007498    0.179430  6.689284\n",
      "86        01F        IEV  0.012909    0.481268  7.553841\n",
      "..        ...        ...       ...         ...       ...\n",
      "303       N8U    LPF,OXH  0.006973    0.045519  2.268294\n",
      "304       OXH    LPF,N8U  0.006973    0.167161  9.919540\n",
      "305   LPF,N8U        OXH  0.006973    0.413761  9.919540\n",
      "306   LPF,OXH        N8U  0.006973    0.347458  2.268294\n",
      "307   OXH,N8U        LPF  0.006973    0.626389  8.715011\n",
      "\n",
      "[166 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df_cleaned = filter_out_lhs(result_df)\n",
    "print(result_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Left_side Right_side   Support  Confidence      Lift\n",
      "305   LPF,N8U        OXH  0.006973    0.413761  9.919540\n",
      "304       OXH    LPF,N8U  0.006973    0.167161  9.919540\n",
      "297       OXH    IEV,N8U  0.006076    0.145663  9.507370\n",
      "298   IEV,N8U        OXH  0.006076    0.396569  9.507370\n",
      "138       FU5        9ZX  0.007220    0.172771  9.100304\n",
      "..        ...        ...       ...         ...       ...\n",
      "104       0H2        N8U  0.009230    0.143544  0.937093\n",
      "233       N8U        U5F  0.007838    0.051171  0.917867\n",
      "234       U5F        N8U  0.007838    0.140599  0.917867\n",
      "231       R6E        N8U  0.008874    0.124620  0.813552\n",
      "230       N8U        R6E  0.008874    0.057933  0.813552\n",
      "\n",
      "[166 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sorting data based on lift (importance)\n",
    "result_df_cleaned = result_df_cleaned.sort_values(by='Lift', ascending=False)\n",
    "print(result_df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The result for task 2.a**  \n",
    "What ‘min_support’ and `min_confidence’ thresholds were set for this mining exercise? Rationalise why these values were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min_support:** We have 187 sku_categories. The chance of a category to occure is 1/187.  \n",
    "**min_confidence:** The maximum confidence is 62%. Even though many rules have confidence less than 50% but they have high value of lift. That says there is still a strong correlation between sku_categories. Therefore we disregarded the min_confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The result for task 2.b**  \n",
    "The top five rules are those with the highest lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Left_side Right_side   Support  Confidence      Lift\n",
      "305   LPF,N8U        OXH  0.006973    0.413761  9.919540\n",
      "304       OXH    LPF,N8U  0.006973    0.167161  9.919540\n",
      "297       OXH    IEV,N8U  0.006076    0.145663  9.507370\n",
      "298   IEV,N8U        OXH  0.006076    0.396569  9.507370\n",
      "138       FU5        9ZX  0.007220    0.172771  9.100304\n"
     ]
    }
   ],
   "source": [
    "print(result_df_cleaned.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "The above shows that the first five with highest lift.  \n",
    "1. LPF,N8U => OXH: Customers who purchase categories LPF and N8U are 9.92 times more likely to also purchase OXH.\n",
    "2. OXH => LPF, N8U: Customers who purchase OXH are 9.92 times more likely to purchase LPF and N8U together despite the low confidence. The high lift shows a strong correlation between purchase of OXH and LPF, N8U.\n",
    "3. OXH => IEV,N8U: Customers who purchase OXH are 9.51 times more likely to also purchase IEV and N8U.\n",
    "4.  IEV,N8U => OXH: Customers who purchase IEV and N8U together are 9.51 times more likely to also purchase OXH.\n",
    "5.  FU5 => 9ZX: Customers who purchase FU5 are 9.10 times more likely to also purchase 9ZX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.3\n",
    "List the five most common product categories that customers bought with the product category '01F'.  \n",
    "\n",
    "To answer this question we need to check occurence of 01F either in antecedent (LHS) or conceqent (RHS). Then sort it by support as the support tells the frequency.  \n",
    "We must be careful duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Left_side Right_side   Support  Confidence      Lift\n",
      "0       01F        IEV  0.012909    0.481268  7.553841\n",
      "1       01F        LPF  0.012198    0.454755  6.327052\n",
      "2       OXH        01F  0.008163    0.195701  7.295851\n",
      "3       01F        FU5  0.007498    0.279539  6.689284\n",
      "4       N8U        01F  0.007421    0.048446  1.806089\n"
     ]
    }
   ],
   "source": [
    "# Get all the rows containing '01F' in both Left_side or Right_side\n",
    "df_01F = result_df_cleaned[(result_df_cleaned['Left_side'].str.contains('01F')) | (result_df_cleaned['Right_side'].str.contains('01F'))]\n",
    "\n",
    "# Sort by Support to find the most common associations\n",
    "df_01F = df_01F.sort_values(by='Support', ascending=False)\n",
    "\n",
    "# Remove duplicates\n",
    "df_01F = df_01F.groupby(lambda x: frozenset([df_01F.at[x, 'Left_side'].strip(), df_01F.at[x, 'Right_side'].strip()])).first().reset_index(drop=True)\n",
    "\n",
    "# The top 5 \n",
    "print(df_01F.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**  \n",
    "{ IEV, LPF, OXH, FU5, N8U }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.4\n",
    "Can you perform sequence analysis on this dataset? If yes, present your results. If not, rationalise why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for task ?: If we focus on date and customer_id, then this will be sequencial as it shows at what sequence a customer purchased what category.\n",
    "Use the jar file for squencial mining. Sort the date first in ascending format. In the result we can see we have order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>sku_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>2547</td>\n",
       "      <td>X52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>TW8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>427</td>\n",
       "      <td>R6E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>427</td>\n",
       "      <td>R6E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>427</td>\n",
       "      <td>Q4N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128099</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>16860</td>\n",
       "      <td>R6E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128100</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>16860</td>\n",
       "      <td>R6E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128101</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>16860</td>\n",
       "      <td>SFC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128091</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>17306</td>\n",
       "      <td>C8Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128234</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>20543</td>\n",
       "      <td>XG4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131706 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  customer_id sku_category\n",
       "0       2016-01-02         2547          X52\n",
       "156     2016-01-02            3          TW8\n",
       "157     2016-01-02          427          R6E\n",
       "158     2016-01-02          427          R6E\n",
       "159     2016-01-02          427          Q4N\n",
       "...            ...          ...          ...\n",
       "128099  2016-12-31        16860          R6E\n",
       "128100  2016-12-31        16860          R6E\n",
       "128101  2016-12-31        16860          SFC\n",
       "128091  2016-12-31        17306          C8Z\n",
       "128234  2016-12-31        20543          XG4\n",
       "\n",
       "[131706 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12 = df1[['Date', 'Customer_ID', 'SKU_Category']]\n",
    "\n",
    "#lowercasing the feature names for ease\n",
    "df12.columns = [col.lower() for col in df12.columns]\n",
    "\n",
    "# The date is in format %d/%m/%Y\n",
    "df12.loc[:, 'date'] = pd.to_datetime(df12['date'], format='%d/%m/%Y').dt.date\n",
    "df12 = df12.sort_values(by='date')\n",
    "\n",
    "df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0H2', 'N8U'],\n",
       " ['TVL', 'F9B'],\n",
       " ['TW8', 'TW8', 'LPF'],\n",
       " ['69B', 'YMJ', '29A', 'N8U', 'JR5'],\n",
       " ['P42', 'P42', 'P42', 'P42', 'LGI'],\n",
       " ['Z23', '1VL', '1EO'],\n",
       " ['1VL'],\n",
       " ['LPF'],\n",
       " ['XG4'],\n",
       " ['SJS']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = df12.groupby('customer_id')['sku_category'].apply(list).tolist()\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0H2', 'N8U'],\n",
       " ['TVL', 'F9B'],\n",
       " ['TW8', 'TW8', 'LPF'],\n",
       " ['69B', 'YMJ', '29A', 'N8U', 'JR5'],\n",
       " ['P42', 'P42', 'P42', 'P42', 'LGI']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing single items\n",
    "sequences = [seq for seq in sequences if len(seq) > 1]\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "''' Uses SPMF to find association rules in supplied transactions '''\n",
    "def get_association_rules(sequences, min_sup, min_conf):\n",
    "    # step 1: create required input for SPMF\n",
    "    \n",
    "    # prepare a dict to uniquely assign each item in the transactions to an int ID\n",
    "    item_dict = defaultdict(int)\n",
    "    output_dict = defaultdict(str)\n",
    "    item_id = 1\n",
    "    \n",
    "    # write your sequences in SPMF format\n",
    "    with open('seq_rule_input.txt', 'w+') as f:\n",
    "        for sequence in sequences:\n",
    "            z = []\n",
    "            for itemset in sequence:\n",
    "                # if there are multiple items in one itemset\n",
    "                if isinstance(itemset, list):\n",
    "                    for item in itemset:\n",
    "                        if item not in item_dict:\n",
    "                            item_dict[item] = item_id\n",
    "                            item_id += 1\n",
    "\n",
    "                        z.append(item_dict[item])\n",
    "                else:\n",
    "                    if itemset not in item_dict:\n",
    "                        item_dict[itemset] = item_id\n",
    "                        output_dict[str(item_id)] = itemset\n",
    "                        item_id += 1\n",
    "                    z.append(item_dict[itemset])\n",
    "                    \n",
    "                # end of itemset\n",
    "                z.append(-1)\n",
    "            \n",
    "            # end of a sequence\n",
    "            z.append(-2)\n",
    "            f.write(' '.join([str(x) for x in z]))\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # run SPMF with supplied parameters\n",
    "    supp_param = '{}%'.format(int(min_sup * 100))\n",
    "    conf_param = '{}%'.format(int(min_conf * 100))\n",
    "    subprocess.call(['java', '-jar', 'spmf.jar', 'run', 'RuleGrowth', \n",
    "                     'seq_rule_input.txt', 'seq_rule_output.txt', \n",
    "                     supp_param, conf_param], shell=True)\n",
    "    \n",
    "    # read back the output rules\n",
    "    outputs = open('seq_rule_output.txt', 'r').read().strip().split('\\n')\n",
    "    output_rules = []\n",
    "    for rule in outputs:\n",
    "        left, right, sup, conf = re.search(pattern=r'([0-9\\,]+) ==> ([0-9\\,]+) #SUP: ([0-9]+) #CONF: ([0-9\\.]+)', string=rule).groups()\n",
    "        sup = int(sup) / len(sequences)\n",
    "        conf = float(conf)\n",
    "        output_rules.append([[output_dict[x] for x in left.split(',')], [output_dict[x] for x in right.split(',')], sup, conf])\n",
    "    \n",
    "    # return pandas DataFrame\n",
    "    return pd.DataFrame(output_rules, columns = ['Left_rule', 'Right_rule', 'Support', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left_rule</th>\n",
       "      <th>Right_rule</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[N8U]</td>\n",
       "      <td>0.059912</td>\n",
       "      <td>0.366567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[IEV]</td>\n",
       "      <td>[N8U]</td>\n",
       "      <td>0.054766</td>\n",
       "      <td>0.377374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[IEV]</td>\n",
       "      <td>0.063771</td>\n",
       "      <td>0.390180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IEV]</td>\n",
       "      <td>[LPF]</td>\n",
       "      <td>0.067998</td>\n",
       "      <td>0.468552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[OXH]</td>\n",
       "      <td>0.051703</td>\n",
       "      <td>0.316342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Left_rule Right_rule   Support  Confidence\n",
       "0     [LPF]      [N8U]  0.059912    0.366567\n",
       "1     [IEV]      [N8U]  0.054766    0.377374\n",
       "2     [LPF]      [IEV]  0.063771    0.390180\n",
       "3     [IEV]      [LPF]  0.067998    0.468552\n",
       "4     [LPF]      [OXH]  0.051703    0.316342"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_13 = get_association_rules(sequences, 0.05, 0.3)\n",
    "df_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above does not show any sequence of purhase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left_rule</th>\n",
       "      <th>Right_rule</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[N8U]</td>\n",
       "      <td>0.059912</td>\n",
       "      <td>0.366567</td>\n",
       "      <td>3.196493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[IEV]</td>\n",
       "      <td>[N8U]</td>\n",
       "      <td>0.054766</td>\n",
       "      <td>0.377374</td>\n",
       "      <td>3.290737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[IEV]</td>\n",
       "      <td>0.063771</td>\n",
       "      <td>0.390180</td>\n",
       "      <td>6.118441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IEV]</td>\n",
       "      <td>[LPF]</td>\n",
       "      <td>0.067998</td>\n",
       "      <td>0.468552</td>\n",
       "      <td>6.890671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[OXH]</td>\n",
       "      <td>0.051703</td>\n",
       "      <td>0.316342</td>\n",
       "      <td>6.118441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Left_rule Right_rule   Support  Confidence      Lift\n",
       "0     [LPF]      [N8U]  0.059912    0.366567  3.196493\n",
       "1     [IEV]      [N8U]  0.054766    0.377374  3.290737\n",
       "2     [LPF]      [IEV]  0.063771    0.390180  6.118441\n",
       "3     [IEV]      [LPF]  0.067998    0.468552  6.890671\n",
       "4     [LPF]      [OXH]  0.051703    0.316342  6.118441"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_14 = df_13.copy()\n",
    "right_rule_supports = df_13.groupby(df_13['Right_rule'].apply(lambda x: tuple(x)))['Support'].sum().to_dict()\n",
    "df_14['Lift'] = df_13.apply(lambda row: row['Confidence'] / right_rule_supports[tuple(row['Right_rule'])], axis=1)\n",
    "df_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left_rule</th>\n",
       "      <th>Right_rule</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[IEV]</td>\n",
       "      <td>[LPF]</td>\n",
       "      <td>0.067998</td>\n",
       "      <td>0.468552</td>\n",
       "      <td>6.890671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[IEV]</td>\n",
       "      <td>0.063771</td>\n",
       "      <td>0.390180</td>\n",
       "      <td>6.118441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[OXH]</td>\n",
       "      <td>0.051703</td>\n",
       "      <td>0.316342</td>\n",
       "      <td>6.118441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[IEV]</td>\n",
       "      <td>[N8U]</td>\n",
       "      <td>0.054766</td>\n",
       "      <td>0.377374</td>\n",
       "      <td>3.290737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[LPF]</td>\n",
       "      <td>[N8U]</td>\n",
       "      <td>0.059912</td>\n",
       "      <td>0.366567</td>\n",
       "      <td>3.196493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Left_rule Right_rule   Support  Confidence      Lift\n",
       "3     [IEV]      [LPF]  0.067998    0.468552  6.890671\n",
       "2     [LPF]      [IEV]  0.063771    0.390180  6.118441\n",
       "4     [LPF]      [OXH]  0.051703    0.316342  6.118441\n",
       "1     [IEV]      [N8U]  0.054766    0.377374  3.290737\n",
       "0     [LPF]      [N8U]  0.059912    0.366567  3.196493"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_14 = df_14.sort_values(by='Lift', ascending=False)\n",
    "df_14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:**  \n",
    "LPF with the confidence range of 18% to 29% is the most important category.  \n",
    "This category has been purchased with IEV, N8U, FU5, and OXH categories.  \n",
    "In addition the high lift value of 7.5% for LPF, indicates that the IEV, N8U, FU5, and OXH 7.5 times more likey to be burchased along with IPF cateogry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A.5\n",
    "How can the outcome of this study be used by the relevant decision-makers?\n",
    "\n",
    "Decision makers can be advised on which product categories are most frequently purchased together.\n",
    "- It also tells which products are correlated\n",
    "- The result may help on various ways:\n",
    "  - increase the purchase intake of the famous products\n",
    "  - reduce the purchase intake of the list famous products\n",
    "  - boundling products for offer (discount) stragety to increase sale\n",
    "  - focus on increasing the sale of products that sell less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
