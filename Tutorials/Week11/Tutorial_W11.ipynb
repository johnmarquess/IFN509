{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Loading the Pre-processed Dataset <a name=\"resume\"></a>\n",
    "\n",
    "We will reuse the code for data preprocessing developed in previous practicals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T00:24:50.239115Z",
     "start_time": "2024-10-17T00:24:50.157329Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from dm_tools import data_prep\n",
    "\n",
    "# set the random seed - consistent\n",
    "rs = 10\n",
    "\n",
    "# load the data\n",
    "df,X,y,X_train, X_test, y_train, y_test = data_prep()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JohnMarquess\\OneDrive - John Marquess\\Projects\\IFN509\\Tutorials\\Week11\\dm_tools.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DemAge'].fillna(df['DemAge'].mean(), inplace=True)\n",
      "C:\\Users\\JohnMarquess\\OneDrive - John Marquess\\Projects\\IFN509\\Tutorials\\Week11\\dm_tools.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['DemMedIncome'].fillna(df['DemMedIncome'].mean(), inplace=True)\n",
      "C:\\Users\\JohnMarquess\\OneDrive - John Marquess\\Projects\\IFN509\\Tutorials\\Week11\\dm_tools.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['GiftAvgCard36'].fillna(df['GiftAvgCard36'].mean(), inplace=True)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the first logistic regression model <a name=\"build\"></a>\n",
    "\n",
    "### 2.1. Standardisation\n",
    "\n",
    "Most machine learning/data mining algorithms, including regression models, are\n",
    "sensitive to input variables on different scales. Input variables on different\n",
    "scales make comparison between data points difficult. For \n",
    "example, consider two variables with different scaled values: student grade (in the\n",
    "range of 1-4) and student age (\\~20-65)). In addition, it also adversely affects **gradient descent** (an algorithm used for training models such as regressions, neural networks, Support Vector Machines, etc) by making weights on larger scale inputs to update much faster than smaller scale inputs, resulting in a suboptimal model performance.\n",
    "\n",
    "> To read more about feature scaling, read this excellent blog post from Sebastian Raschka [link](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "\n",
    "To avoid this problem, it is necessary to perform standardisation on input variables."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T00:26:42.998172Z",
     "start_time": "2024-10-17T00:26:42.977706Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      TargetB  GiftCnt36  GiftCntAll  GiftCntCard36  GiftCntCardAll  \\\n",
       "0           0          2           4              1               3   \n",
       "1           0          1           8              0               3   \n",
       "2           1          6          41              3              20   \n",
       "3           1          3          12              3               8   \n",
       "4           0          1           1              1               1   \n",
       "...       ...        ...         ...            ...             ...   \n",
       "9681        1          2           2              2               2   \n",
       "9682        0          3           4              1               1   \n",
       "9683        1          9          14              5               7   \n",
       "9684        0          2           7              0               1   \n",
       "9685        0          8          15              7              10   \n",
       "\n",
       "      GiftAvgLast  GiftAvg36  GiftAvgAll  GiftAvgCard36  GiftTimeLast  ...  \\\n",
       "0            17.0      13.50        9.25      17.000000            21  ...   \n",
       "1            20.0      20.00       15.88      14.224431            26  ...   \n",
       "2             6.0       5.17        3.73       5.000000            18  ...   \n",
       "3            10.0       8.67        8.50       8.670000             9  ...   \n",
       "4            20.0      20.00       20.00      20.000000            21  ...   \n",
       "...           ...        ...         ...            ...           ...  ...   \n",
       "9681         10.0      15.00       15.00      15.000000            17  ...   \n",
       "9682         10.0      13.33       11.50      10.000000            25  ...   \n",
       "9683         10.0       8.78        7.71       8.800000            15  ...   \n",
       "9684          5.0      12.50        7.43      14.224431             6  ...   \n",
       "9685          5.0       4.88        4.53       5.140000            22  ...   \n",
       "\n",
       "      DemCluster_51  DemCluster_52  DemCluster_53  DemCluster_6  DemCluster_7  \\\n",
       "0             False          False          False         False         False   \n",
       "1             False          False          False         False         False   \n",
       "2             False          False          False         False         False   \n",
       "3             False          False          False         False         False   \n",
       "4             False          False          False         False         False   \n",
       "...             ...            ...            ...           ...           ...   \n",
       "9681          False          False          False         False         False   \n",
       "9682          False          False          False         False         False   \n",
       "9683          False          False          False         False         False   \n",
       "9684          False          False          False         False         False   \n",
       "9685          False          False          False         False         False   \n",
       "\n",
       "      DemCluster_8  DemCluster_9  DemGender_F  DemGender_M  DemGender_U  \n",
       "0            False         False         True        False        False  \n",
       "1            False         False         True        False        False  \n",
       "2            False         False        False         True        False  \n",
       "3            False         False        False         True        False  \n",
       "4            False         False        False         True        False  \n",
       "...            ...           ...          ...          ...          ...  \n",
       "9681         False         False        False         True        False  \n",
       "9682         False         False        False        False         True  \n",
       "9683         False         False         True        False        False  \n",
       "9684         False         False        False         True        False  \n",
       "9685         False         False         True        False        False  \n",
       "\n",
       "[9686 rows x 86 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TargetB</th>\n",
       "      <th>GiftCnt36</th>\n",
       "      <th>GiftCntAll</th>\n",
       "      <th>GiftCntCard36</th>\n",
       "      <th>GiftCntCardAll</th>\n",
       "      <th>GiftAvgLast</th>\n",
       "      <th>GiftAvg36</th>\n",
       "      <th>GiftAvgAll</th>\n",
       "      <th>GiftAvgCard36</th>\n",
       "      <th>GiftTimeLast</th>\n",
       "      <th>...</th>\n",
       "      <th>DemCluster_51</th>\n",
       "      <th>DemCluster_52</th>\n",
       "      <th>DemCluster_53</th>\n",
       "      <th>DemCluster_6</th>\n",
       "      <th>DemCluster_7</th>\n",
       "      <th>DemCluster_8</th>\n",
       "      <th>DemCluster_9</th>\n",
       "      <th>DemGender_F</th>\n",
       "      <th>DemGender_M</th>\n",
       "      <th>DemGender_U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>9.25</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.88</td>\n",
       "      <td>14.224431</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.17</td>\n",
       "      <td>3.73</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.67</td>\n",
       "      <td>8.50</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9682</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>11.50</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9683</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9684</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.50</td>\n",
       "      <td>7.43</td>\n",
       "      <td>14.224431</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.53</td>\n",
       "      <td>5.140000</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9686 rows × 86 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T00:24:55.443947Z",
     "start_time": "2024-10-17T00:24:55.367826Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# initialise a standard scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# visualise min, max, mean and standard dev of data before scaling\n",
    "print(\"Before scaling\\n-------------\")\n",
    "for i in range(5):\n",
    "    col = X_train[:,i]\n",
    "    print(\"Variable #{}: min {}, max {}, mean {:.2f} and std dev {:.2f}\".\n",
    "          format(i, min(col), max(col), np.mean(col), np.std(col)))\n",
    "\n",
    "# learn the mean and std.dev of variables from training data\n",
    "# then use the learned values to transform training data\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "\n",
    "print(\"After scaling\\n-------------\")\n",
    "for i in range(5):\n",
    "    col = X_train[:,i]\n",
    "    print(\"Variable #{}: min {}, max {}, mean {:.2f} and std dev {:.2f}\".\n",
    "          format(i, min(col), max(col), np.mean(col), np.std(col)))\n",
    "\n",
    "# use the statistic that you learned from training to transform test data\n",
    "# NEVER learn from test data, this is supposed to be a set of dataset\n",
    "# that the model has never seen before\n",
    "X_test = scaler.transform(X_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling\n",
      "-------------\n",
      "Variable #0: min 0, max 13, mean 3.20 and std dev 2.13\n",
      "Variable #1: min 1, max 89, mean 10.50 and std dev 8.95\n",
      "Variable #2: min 0, max 9, mean 1.86 and std dev 1.59\n",
      "Variable #3: min 0, max 41, mean 5.58 and std dev 4.73\n",
      "Variable #4: min 0.0, max 450.0, mean 15.97 and std dev 12.07\n",
      "After scaling\n",
      "-------------\n",
      "Variable #0: min -1.505311488472091, max 4.600729037733905, mean -0.00 and std dev 1.00\n",
      "Variable #1: min -1.0623391496250907, max 8.7746107562059, mean -0.00 and std dev 1.00\n",
      "Variable #2: min -1.1659811623481273, max 4.487429288703341, mean -0.00 and std dev 1.00\n",
      "Variable #3: min -1.1795617205202689, max 7.485041166941727, mean -0.00 and std dev 1.00\n",
      "Variable #4: min -1.3225650969557856, max 35.95505089608766, mean 0.00 and std dev 1.00\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Training logistic regression\n",
    "\n",
    "Once the input variables are scaled, we are ready to build the model. As\n",
    "discussed before, the underlying problem is classification predictive mining,\n",
    "therefore, a logistic regression model will be build for this classification\n",
    "task. In sklearn, logistic regression is implemented in\n",
    "`sklearn.linear_model.LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to decision tree, initiate a model object (with random state to ensure consistent result) and fit it to the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=10, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=rs)\n",
    "\n",
    "# fit it to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5936578171091446\n",
      "Test accuracy: 0.56194081211287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57      1453\n",
      "           1       0.56      0.55      0.56      1453\n",
      "\n",
      "    accuracy                           0.56      2906\n",
      "   macro avg       0.56      0.56      0.56      2906\n",
      "weighted avg       0.56      0.56      0.56      2906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "# classification report on test data\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not produce an acceptable accuracy score on training data. The training accuracy is slightly higher than the test accuracy which may indicate slight overfitting but that needs to be investigated. We will tune this logistic regression model later using GridSearchCV to imrprove the accuracy.\n",
    "\n",
    "## 3. Understanding the logistic regression model <a name=\"viz\"></a>\n",
    "Once the model is trained, all of its weights are stored in `.coef_` array of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.56479317e-02  5.74631371e-02  9.58946027e-02 -7.69919083e-02\n",
      "  -5.02902331e-02 -3.99709024e-02  7.37732317e-02 -5.92560335e-02\n",
      "  -1.68292677e-01  2.80519442e-01 -1.39754466e-01  5.05070065e-02\n",
      "   1.97683503e-01  4.77612140e-02  9.50116479e-02 -4.38436665e-01\n",
      "   1.10063136e-01  5.51064382e-02  2.69872103e-02  1.25814151e-01\n",
      "   4.53440991e-02 -2.50488747e-02 -2.30633992e-02  7.76561114e-02\n",
      "  -2.66603966e-02 -7.16675987e-03  2.82366134e-02  5.92501486e-05\n",
      "   4.95165519e-02  2.50354442e-02 -5.18832419e-02  2.46946960e-02\n",
      "   2.03374611e-03  4.35110123e-03 -1.70204184e-04 -1.34981177e-03\n",
      "  -2.41930044e-02  1.54508038e-02  2.44619453e-02 -5.98428132e-04\n",
      "   9.42231360e-03  1.79364746e-02 -3.78661229e-02 -1.01825582e-02\n",
      "  -1.30017508e-02  1.83649892e-02 -7.88115883e-03  8.93770163e-04\n",
      "  -3.88925245e-04  1.77522198e-02  1.19328388e-02  2.45900873e-02\n",
      "  -5.11924297e-02  1.11579624e-02 -4.77995044e-02 -1.00049707e-02\n",
      "  -3.03188210e-03  3.13878856e-02 -1.14417950e-03 -1.78740259e-02\n",
      "   3.76321854e-02  1.77111033e-02  2.15422673e-02  4.20480033e-02\n",
      "  -3.26692890e-02  4.23645062e-02 -1.14815004e-02 -2.38148921e-02\n",
      "  -4.50369881e-02  4.15033193e-02 -5.28391255e-02 -8.82257230e-03\n",
      "  -2.79028176e-02 -1.95684861e-02  7.06629557e-03 -1.79374750e-02\n",
      "  -3.52771549e-02  2.68346487e-02 -3.67798085e-02  8.25476912e-03\n",
      "  -2.19041951e-02 -2.22344683e-02  5.20549752e-03  1.87470616e-03\n",
      "  -1.53770271e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these values corresponds to a variable/feature in the dataset. We can print out the feature name associated with each coefficient with this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftCnt36 : 0.08564793174330508\n",
      "GiftCntAll : 0.05746313707979046\n",
      "GiftCntCard36 : 0.09589460268134936\n",
      "GiftCntCardAll : -0.07699190828076483\n",
      "GiftAvgLast : -0.05029023311749633\n",
      "GiftAvg36 : -0.03997090240216672\n",
      "GiftAvgAll : 0.07377323168569479\n",
      "GiftAvgCard36 : -0.05925603345643192\n",
      "GiftTimeLast : -0.16829267718061125\n",
      "GiftTimeFirst : 0.28051944167333015\n",
      "PromCnt12 : -0.13975446633584174\n",
      "PromCnt36 : 0.05050700651424271\n",
      "PromCntAll : 0.1976835033755152\n",
      "PromCntCard12 : 0.04776121404026222\n",
      "PromCntCard36 : 0.0950116479196176\n",
      "PromCntCardAll : -0.4384366648540792\n",
      "StatusCatStarAll : 0.11006313623278328\n",
      "DemAge : 0.055106438177314344\n",
      "DemHomeOwner : 0.026987210346360868\n",
      "DemMedHomeValue : 0.1258141510220739\n"
     ]
    }
   ],
   "source": [
    "feature_names = X.columns\n",
    "coef = model.coef_[0]\n",
    "\n",
    "# limit to 20 features, you can comment the following line to print out everything\n",
    "coef = coef[:20]\n",
    "\n",
    "for i in range(len(coef)):\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall again that a linear/logistic regression model assumes a linear correlation between input features and target values. This relationship is reflected in the weight values.\n",
    "* Positive coefficient means positive change in the input feature has positive correlation to the prediction value. **DemAge** has positive coefficients, which means older donors (larger age value) are predicted to be more likely to donate back.\n",
    "\n",
    "* Negative coefficient does the reverse. For example, **GiftTimeLast** has negative coefficient, thus if a donor has not donated for a while, the model predicts it is less likely for them to be a lapsing donor.\n",
    "\n",
    "In addition to understanding positive/negative correlations between features and prediction values, it is important to learn how much a feature impacts the overall prediction value (i.e. feature importance). One way to answer this question is by looking at the absolute value of coefficients. Changes in an important variable (either positive or negative) should correlate to a larger impact to prediction value, thus the coefficient assigned to this variable will have a large absolute value. \n",
    "\n",
    "This method only applies when the features are standard scaled. Without scaling, each feature can be on different range of values, making the larger ranged feature more important by default. For example, in a dataset with **age** and **income**, a **+10** increase in age is significant, but it is meaningless in income. \n",
    "\n",
    "The following code will sort the coefficients by their largest absolute value and print the corresponding feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromCntCardAll : -0.4384366648540792\n",
      "GiftTimeFirst : 0.28051944167333015\n",
      "PromCntAll : 0.1976835033755152\n",
      "GiftTimeLast : -0.16829267718061125\n",
      "PromCnt12 : -0.13975446633584174\n",
      "DemMedHomeValue : 0.1258141510220739\n",
      "StatusCatStarAll : 0.11006313623278328\n",
      "GiftCntCard36 : 0.09589460268134936\n",
      "PromCntCard36 : 0.0950116479196176\n",
      "GiftCnt36 : 0.08564793174330508\n",
      "StatusCat96NK_E : 0.07765611138128718\n",
      "GiftCntCardAll : -0.07699190828076483\n",
      "GiftAvgAll : 0.07377323168569479\n",
      "GiftAvgCard36 : -0.05925603345643192\n",
      "GiftCntAll : 0.05746313707979046\n",
      "DemAge : 0.055106438177314344\n",
      "DemCluster_47 : -0.05283912552726659\n",
      "DemCluster_10 : -0.05188324186791754\n",
      "DemCluster_30 : -0.051192429695869596\n",
      "PromCnt36 : 0.05050700651424271\n"
     ]
    }
   ],
   "source": [
    "# grab feature importances from the model and feature name from the original X\n",
    "coef = model.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(np.absolute(coef))\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:20]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', coef[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, the top-3 important variables in this model are PromCntCardAll, GiftTimeFirst, and PromCntAll. The ordering of important variables may differ different between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding optimal hyperparameters with GridSearchCV <a name=\"gridsearch\"></a>\n",
    "\n",
    "This section will guide you to tune the default model to perform better. In this logistic regression model, we will only tune one hyperparameter,  the regularisation strength. Regularisation is a technique used to prevent overfitting in regression models. We will not go in depth to explain how regularisation works, you can find more information here:\n",
    "* [Basic of regularisation](https://www.quora.com/What-is-regularization-in-machine-learning)\n",
    "* [L1 vs L2 regularisation](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)\n",
    "\n",
    "In `sklearn` logistic regression, regularisation is implemented via the hyperparameter **C**, which denotes the inverse of regularisation strength. Smaller C means stronger regularisation. Typical values for C range from $10^{-6}$ to $10^{4}$, increasing in logarithmic order, which are the values we will use in this example.\n",
    "\n",
    "**Tips:** `GridSearchCV` can be slow for searching over a large set of possible values. To speed up the searching process, `GridSearchCV` has parallel running capability, where you can specify how many parallel processes run concurrently with `n_jobs` (-1 means GridSearchCV will use as many cores as possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=10, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10,\n",
       "                               100, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "# use all cores to tune logistic regression with C parameter\n",
    "cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs),return_train_score=True, cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.10142791, 0.07161188, 0.14961674, 0.16470158, 0.3754668 ,\n",
      "       0.55314081, 0.83623943, 0.7572623 , 0.66558859, 0.53418951]), 'std_fit_time': array([0.04945515, 0.01910362, 0.03351158, 0.05139281, 0.13451814,\n",
      "       0.14011479, 0.35983689, 0.25231347, 0.19691503, 0.09289368]), 'mean_score_time': array([0.00292583, 0.00299695, 0.00249174, 0.00279195, 0.00209129,\n",
      "       0.00422626, 0.00276341, 0.00235765, 0.00257745, 0.00296035]), 'std_score_time': array([0.00205444, 0.00285807, 0.0014974 , 0.00203569, 0.00094485,\n",
      "       0.0051413 , 0.0016275 , 0.00117242, 0.00150397, 0.00139077]), 'param_C': masked_array(data=[1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100,\n",
      "                   1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1e-06}, {'C': 1e-05}, {'C': 0.0001}, {'C': 0.001}, {'C': 0.01}, {'C': 0.1}, {'C': 1}, {'C': 10}, {'C': 100}, {'C': 1000}], 'split0_test_score': array([0.57522124, 0.57817109, 0.58702065, 0.59144543, 0.60176991,\n",
      "       0.60324484, 0.59882006, 0.59882006, 0.59882006, 0.59882006]), 'split1_test_score': array([0.5840708 , 0.58554572, 0.59587021, 0.59882006, 0.59439528,\n",
      "       0.59144543, 0.58849558, 0.5899705 , 0.5899705 , 0.5899705 ]), 'split2_test_score': array([0.56489676, 0.57079646, 0.59144543, 0.60914454, 0.60176991,\n",
      "       0.59882006, 0.60176991, 0.60029499, 0.60029499, 0.60029499]), 'split3_test_score': array([0.53097345, 0.52949853, 0.53244838, 0.55014749, 0.55457227,\n",
      "       0.55309735, 0.54867257, 0.54867257, 0.54867257, 0.54867257]), 'split4_test_score': array([0.59292035, 0.58849558, 0.57079646, 0.58259587, 0.5840708 ,\n",
      "       0.58702065, 0.58259587, 0.58702065, 0.58702065, 0.58702065]), 'split5_test_score': array([0.56932153, 0.57669617, 0.58112094, 0.59882006, 0.59144543,\n",
      "       0.58554572, 0.58259587, 0.58259587, 0.58259587, 0.58259587]), 'split6_test_score': array([0.55457227, 0.55457227, 0.55899705, 0.54572271, 0.54424779,\n",
      "       0.55014749, 0.55162242, 0.55162242, 0.55162242, 0.55162242]), 'split7_test_score': array([0.5560472 , 0.56342183, 0.57669617, 0.58702065, 0.57374631,\n",
      "       0.57227139, 0.56784661, 0.56932153, 0.56932153, 0.56784661]), 'split8_test_score': array([0.57522124, 0.57817109, 0.57227139, 0.58702065, 0.56784661,\n",
      "       0.56637168, 0.56932153, 0.56784661, 0.56784661, 0.56784661]), 'split9_test_score': array([0.54277286, 0.54424779, 0.56489676, 0.56784661, 0.56489676,\n",
      "       0.56047198, 0.56047198, 0.56047198, 0.56047198, 0.56047198]), 'mean_test_score': array([0.56460177, 0.56696165, 0.57315634, 0.58185841, 0.57787611,\n",
      "       0.57684366, 0.57522124, 0.57566372, 0.57566372, 0.57551622]), 'std_test_score': array([0.01791656, 0.01804239, 0.01746651, 0.01993334, 0.01896643,\n",
      "       0.01802128, 0.01764988, 0.01778556, 0.01778556, 0.01784356]), 'rank_test_score': array([10,  9,  8,  1,  2,  3,  7,  4,  4,  6]), 'split0_train_score': array([0.5696493 , 0.57194363, 0.58030154, 0.5942314 , 0.59488692,\n",
      "       0.59439528, 0.59488692, 0.59472304, 0.59472304, 0.59472304]), 'split1_train_score': array([0.56948541, 0.57014094, 0.57817109, 0.59062602, 0.59111767,\n",
      "       0.59062602, 0.59062602, 0.59013438, 0.59029826, 0.59029826]), 'split2_train_score': array([0.56866601, 0.57063258, 0.5763684 , 0.59111767, 0.59652573,\n",
      "       0.59095379, 0.59390364, 0.59390364, 0.59373976, 0.59373976]), 'split3_train_score': array([0.57341855, 0.57456572, 0.5806293 , 0.59521468, 0.59652573,\n",
      "       0.5942314 , 0.59292035, 0.59324812, 0.59308423, 0.59308423]), 'split4_train_score': array([0.57030482, 0.57079646, 0.58144871, 0.59554245, 0.59455916,\n",
      "       0.59390364, 0.59537856, 0.59537856, 0.59537856, 0.59521468]), 'split5_train_score': array([0.56817437, 0.56915765, 0.58030154, 0.59144543, 0.59455916,\n",
      "       0.59357588, 0.59357588, 0.59324812, 0.59308423, 0.59324812]), 'split6_train_score': array([0.57227139, 0.57325467, 0.58292363, 0.5984923 , 0.59783677,\n",
      "       0.59816454, 0.60045887, 0.60078663, 0.60078663, 0.60078663]), 'split7_train_score': array([0.57112422, 0.57112422, 0.57686005, 0.59652573, 0.5993117 ,\n",
      "       0.59882006, 0.59914782, 0.59914782, 0.59898394, 0.59898394]), 'split8_train_score': array([0.5712881 , 0.57177974, 0.5798099 , 0.59636185, 0.60078663,\n",
      "       0.59668961, 0.59537856, 0.59554245, 0.59554245, 0.59554245]), 'split9_train_score': array([0.57227139, 0.57456572, 0.58292363, 0.60127827, 0.59898394,\n",
      "       0.59816454, 0.59947558, 0.59980334, 0.59980334, 0.59980334]), 'mean_train_score': array([0.57066536, 0.57179613, 0.57997378, 0.59508358, 0.59650934,\n",
      "       0.59495247, 0.59557522, 0.59559161, 0.59554245, 0.59554245]), 'std_train_score': array([0.00161537, 0.00173195, 0.00214928, 0.00321707, 0.0027082 ,\n",
      "       0.00277748, 0.00301184, 0.00319046, 0.00317861, 0.00316761])}\n"
     ]
    }
   ],
   "source": [
    "result_set = cv.cv_results_\n",
    "print(result_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Decision tree, there were three hyperparameters fine-tuned using GridSearchCV. In logistic regression, `C` is the only hyperparameter we will fine-tune. Therefore, the number of models built is equal to the range of values we define for the hyperparameter `C`. \n",
    "\n",
    "Let us plot the train and test score of split0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models:  10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_result = result_set['split0_train_score']\n",
    "test_result = result_set['split0_test_score']\n",
    "print(\"Total number of models: \", len(test_result))\n",
    "# plot Hyperparameter C values vs training and test accuracy score\n",
    "plt.plot(range(0, len(train_result)), train_result, 'b', range(0,len(test_result)), test_result, 'r')\n",
    "plt.xlabel('Hyperparameter C\\nBlue = training acc. Red = test acc.')\n",
    "plt.xticks(range(0, len(train_result)),[pow(10, x) for x in range(-6, 4)])\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, the optimal value for `C` is 0.1. However this will not be the final choice as we have 9 more splits to consider. Now, let us plot the mean train and test score for all the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models:  10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_result = result_set['mean_train_score']\n",
    "test_result = result_set['mean_test_score']\n",
    "print(\"Total number of models: \", len(test_result))\n",
    "# plot Hyperparameter C values vs training and test accuracy score\n",
    "plt.plot(range(0, len(train_result)), train_result, 'b', range(0,len(test_result)), test_result, 'r')\n",
    "plt.xlabel('Hyperparameter C\\nBlue = training acc. Red = test acc.')\n",
    "plt.xticks(range(0, len(train_result)),[pow(10, x) for x in range(-6, 4)])\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure it can be noted that the default logistic regression model with parameter `C=1` overfits. This figure ascertains that setting `C > 0.001` causes the model to be overfiting (the test error beyond this point degrades regrardless of slight improvement in training accuracy error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model provides the best test score with `C = 0.001`, it is seleceted as the optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5949852507374631\n",
      "Test accuracy: 0.5650378527185134\n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature selection <a name=\"fselect\"></a>\n",
    "\n",
    "Other than fine-tuning models by performing grid search over the hyperparameters, there are many methods to improve prediction quality of a model. One way is to reduce the size of input set expecting that it will improve model performance, reduce overfitting and enhance visualiation. \n",
    "\n",
    "Dimensionality reduction can bedivided into two processes:\n",
    "* Feature selection: Process of selecting a subset of relevant features (variables) to be used in constructing models.\n",
    "* Feature extraction: Process of transforming the high-dimensional feature space into a lower dimension. Typically performed by finding principle components of the feature space.\n",
    "\n",
    "### 5.1. Feature selection using Recursive Feature Elimination\n",
    "\n",
    "Let us explore the dimensionality reduction technique called recursive feature elimination (RFE). RFE works by first training the model on all features. Each feature is assigned a weight. Features with small weights (less important) are eliminated, making a smaller feature set. This process is repeated a number of times until reaching the optimal performance.\n",
    "\n",
    "In this practical, we will use RFE with cross validation (CV). Cross validation allows RFE to generalise better over a set of training datasets. RFE with CV is implemented in sklearn under `sklearn.feature_selection.RFECV`. Initiate the RFE with a logistic regression estimator and 10-fold CV to start eliminating features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature set 85\n",
      "Number of features after elimination 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs), cv=10)\n",
    "rfe.fit(X_train, y_train) # run the RFECV\n",
    "\n",
    "# comparing how many variables before and after\n",
    "print(\"Original feature set\", X_train.shape[1])\n",
    "print(\"Number of features after elimination\", rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE+CV process outputs a total of 40 features as the optimal number of features. In other words, the regression model identifies the 40 input variables to be important to make a prediction. We can now .transform() the original input set taking only the important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sel = rfe.transform(X_train)\n",
    "X_test_sel = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run another `GridSearchCV` and test if the new input set can improve the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5936578171091446\n",
      "Test accuracy: 0.5688231245698555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.58      0.57      1453\n",
      "           1       0.57      0.56      0.56      1453\n",
      "\n",
      "    accuracy                           0.57      2906\n",
      "   macro avg       0.57      0.57      0.57      2906\n",
      "weighted avg       0.57      0.57      0.57      2906\n",
      "\n",
      "{'C': 100}\n"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "rfe_cv = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "rfe_cv.fit(X_train_sel, y_train)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", rfe_cv.score(X_train_sel, y_train))\n",
    "print(\"Test accuracy:\", rfe_cv.score(X_test_sel, y_test))\n",
    "\n",
    "y_pred = rfe_cv.predict(X_test_sel)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(rfe_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RFE only managed to improve the accuracy of the model slightly on the test data. More importantly, with the much smaller feature set, the training, testing and prediction process is speed up significantly. Additionally, the smaller model enhances the decision making process of teh model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Feature selection using another model\n",
    "\n",
    "We will explore another mdethod of feature selection \"select from model\". A machine learning model with the ability to find feature importance can be used to select features with high importance. Typically, decision tree or support vector machine models are used in this method. Recall from the previous practical that decision trees can compute feature importance internally through the tree structure. Therefore, we will use the best decision tree as \"selecting model\" to use the high quality subset of variables.\n",
    "\n",
    "Tip: You can use `pickle.load` to load the `DT.pickle` that we have build as part of the decision tree practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('DT.pickle', 'rb') as f:\n",
    "    dt_best,roc_index_dt_cv, fpr_dt_cv, tpr_dt_cv = pickle.load(f)\n",
    "#dt_best.fit(X_train, y_train)\n",
    "\n",
    "print(dt_best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the feature importance from the trained decision tree model using `analyse_feature_importance()` function (we have written this code in the decison tree mining practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GiftCnt36 : 0.2786889679489785\n",
      "DemMedHomeValue : 0.16527761052028647\n",
      "GiftAvgLast : 0.12080196710144986\n",
      "GiftCntAll : 0.06936170003476945\n",
      "GiftTimeLast : 0.06852490725467034\n",
      "StatusCatStarAll : 0.039760012572680054\n",
      "DemAge : 0.0395320434572485\n",
      "PromCnt36 : 0.034873357233237874\n",
      "GiftCntCardAll : 0.030230871689451603\n",
      "GiftTimeFirst : 0.027615727171569734\n",
      "DemPctVeterans : 0.025370100839886917\n",
      "PromCnt12 : 0.021248703587791955\n",
      "DemCluster_21 : 0.016907242527362424\n",
      "PromCntCard36 : 0.016068732710812456\n",
      "PromCntAll : 0.01598797726061107\n",
      "PromCntCard12 : 0.015718616757198205\n",
      "DemMedIncome : 0.014031461331994594\n",
      "DemCluster_48 : 0.0\n",
      "StatusCat96NK_L : 0.0\n",
      "StatusCat96NK_A : 0.0\n"
     ]
    }
   ],
   "source": [
    "from dm_tools import analyse_feature_importance\n",
    "\n",
    "# analyse feature importance from the tuned decision tree\n",
    "analyse_feature_importance(dt_best.best_estimator_, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows there are only 16 features with importance value more than 0. According to the tuned decision tree, there are only 16 important features in this dataset for prediction. This decision tree can be used to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `SelectFromModel` module from `sklearn.feature_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6780, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# use the trained best decision tree from GridSearchCV to select features\n",
    "# supply the prefit=True parameter to stop SelectFromModel to re-train the model\n",
    "selectmodel = SelectFromModel(dt_best.best_estimator_, prefit=True)\n",
    "X_train_sel_model = selectmodel.transform(X_train)\n",
    "X_test_sel_model = selectmodel.transform(X_test)\n",
    "\n",
    "print(X_train_sel_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of X_train shows that only 17 feature left what the decision tree suggests. Next, train and tune another logistic regression model from this new data set and see if it improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5880530973451328\n",
      "Test accuracy: 0.5660701995870613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.58      0.57      1453\n",
      "           1       0.57      0.55      0.56      1453\n",
      "\n",
      "    accuracy                           0.57      2906\n",
      "   macro avg       0.57      0.57      0.57      2906\n",
      "weighted avg       0.57      0.57      0.57      2906\n",
      "\n",
      "{'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "### For the params given, build a logistic regression model with GridSearch.\n",
    "\n",
    "cv_sel_model = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv_sel_model.fit(X_train_sel_model, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv_sel_model.score(X_train_sel_model, y_train))\n",
    "print(\"Test accuracy:\", cv_sel_model.score(X_test_sel_model, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv_sel_model.predict(X_test_sel_model)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv_sel_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no noticeable improvement in the accuracy performance as compared to the previously build models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparision and finding the best performing model<a name=\"compare\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of four models has been built:\n",
    "    1. Default logistic regression (`model`)\n",
    "    2. Logistic regression + grid search (`cv`)\n",
    "    3. Logistic regression + feature selection using RFE + grid search (`rfe_cv`)\n",
    "    4. Logistic regression + feature selection using DT + grid search (`cv_sel_model`)\n",
    "    \n",
    "Now, use ROC curve to compare these models along with the best performing decision tree and identify the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code \n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save your output <a name=\"save\"></a>\n",
    "\n",
    "Next, save the ROC outputs of the best performing logistic regression model so that we can use it to compare with the Neural Network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter your code\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End notes\n",
    "In this practical, we learned how to build and tune logistic regression models. We explored dimensionality reduction to reduce the size of the feature set and improve the performance of models. In addition, we used ROC curves to compare end-to-end performance of all models we have\n",
    "built so far. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
