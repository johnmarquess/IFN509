{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Computer Tutorial\n",
    "######  Dr Richi Nayak, r.nayak@qut.edu.au\n",
    "\n",
    "**Tutorial Topics:**\n",
    "1. [Association mining Introduction: Basics](#amib)\n",
    "2. [Measures of interestingness of Association rules](#moiar)\n",
    "3. [Apriori Algorithm](#ap)\n",
    "\n",
    "**Practical Topics:**\n",
    "1. [Performing Association Mining](#am)\n",
    "2. [Performing Sequential Rule Mining Using SPMF](#sm)\n",
    "\n",
    "\n",
    "# Part 1 - Reflective exercises\n",
    "In the past weeks, you have learned the data exploration using summary statistics, visualization and data preparation. It helps to understand and prepare the dataset for the data mining models as shown in the following diagram. In this practical, you will be introduced to a new dataset, data preparation for association mining, and association and sequnce rule mining. Please reflect on the association mining topic and answer the questions provided in each subsection.\n",
    "\n",
    "<img src=\"ProcessFlow.png\" width=700 height=400 />\n",
    "\n",
    "## Exercise 1: Association mining Introduction: Basics<a name=\"amib\"></a>\n",
    "\n",
    "\n",
    "1. A data mining algorithm designed to discover frequently accessed items that occur in the same order.\n",
    "\n",
    "        a. serial miner\n",
    "        b. association rule miner\n",
    "        c. sequence miner\n",
    "        d. decision miner \n",
    "\n",
    "2. For the given dataset, identify how many items, itemset and transactions are there.\n",
    "<img src=\"TransactionDB.png\" width=300 height=100 />\n",
    "\n",
    "3. Unlike traditional decision rules, association rules \n",
    "\n",
    "        a. allow the same variable to be an input attribute in one rule and an output attribute in another rule.    \n",
    "        b. allow more than one input attribute in a single rule.\n",
    "        c. require input attributes to take on numeric values. \n",
    "        d. require each rule to have exactly one categorical output attribute. \n",
    "\n",
    "4. This approach is best when we are interested in finding all possible interactions among a set of attributes.\n",
    "\n",
    "        a. Decision tree\n",
    "        b. Association rules\n",
    "        c. K-Means algorithm\n",
    "        d. Regression function\n",
    "\n",
    "## Exercise 2: Measures of interestingness of Association rules<a name=\"moiar\"></a>\n",
    "\n",
    "1. From the given example association rules, identify the invalid ones and explain why. \n",
    "\n",
    "        a. A,B => E,C\n",
    "        b. A => B,C\n",
    "        c. A,B => B,C\n",
    "        d. A,B => {}\n",
    "        e. {} => C,E\n",
    "    \n",
    "2. Given the following association rules result, interpret what does it mean? \n",
    "Diaper, Milk→ Beer [support =5%, confidence=80%] \n",
    "\n",
    "3. Consider the following transactional data\n",
    "<img src=\"TransactionDB1.png\" width=300 height=100 />\n",
    "Answer the following questions:\n",
    "\n",
    "        a. Compute the support for itemsets {A}, {B}, {C}, and {A, B}\n",
    "        b. Compute the confidence and lift for the rules {A, C} -> B and {B, C} -> A. \n",
    "         \n",
    "4. Association rule `support` is defined as\n",
    "\n",
    "        a. the percentage of instances that contain the antecendent conditional items listed in the association rule. \n",
    "        b. the percentage of instances that contain the consequent conditions listed in the association rule. \n",
    "        c. the percentage of instances that contain all items listed in the association rule. \n",
    "        d. the percentage of instances in the database that contain at least one of the antecendent conditional items listed in the association rule. \n",
    "\n",
    "5. Given a rule of the form IF X THEN Y, rule confidence is defined as the conditional probability that\n",
    "\n",
    "        a. Y is true when X is known to be true.\n",
    "        b. X is true when Y is known to be true.\n",
    "        c. Y is false when X is known to be false.\n",
    "        d. X is false when Y is known to be false.\n",
    "    \n",
    "6. Calculate the confidence of rules A → BCD, and ABC → D given their support?\n",
    "\n",
    "7. Given a frequent itemset (ABCD), \n",
    "   - Generate all its frequent 3-itemset subsets.\n",
    "   - Generate all the association rules with three items on LHS and one item on RHS?\n",
    "\n",
    "\n",
    "## Exercise 3: Apriori Algorithm<a name=\"ap\"></a>\n",
    "1. Suppose you use the Apriori algorithm on the dataset with a minimum support threshold of 20%. How many candidate and frequent itemsets will be generated? Generate at least 10 association rules assuming that the confidence threshold is at 50%. \n",
    "<img src=\"TransactionDB2.png\" width=300 height=100 />\n",
    "\n",
    "2. If the minimum support threshold is zero, how many candidate itemsets must be generated by the algorithm?\n",
    "\n",
    "3. How many candidate items are generated in the first scan of the algorithm?\n",
    "\n",
    "4. Consider an example of a supermarket database which might have several million transactions and several thousand items of which only 1000 items are frequent. Which part of the Apriori algorithm will be most expensive to compute and Why?\n",
    "\n",
    "5. Given this transactional data, identify the problems that will be faced with an association mining algorithm.\n",
    "\n",
    "| Transaction Id |Items Bought|\n",
    "| --- | --- |\n",
    "| 001 |{A}|\n",
    "| 002 |{A}|\n",
    "| 003 |{B,C}|\n",
    "| 004 |{D}|\n",
    "| 005 |{A}|\n",
    "| 006 |{D}|\n",
    "| 007 |{A}|\n",
    "| 008 |{A}|\n",
    "| 009 |{A}|\n",
    "| 010 |{B,B}|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Practical exercises\n",
    "___\n",
    "A transactional dataset called `bank.csv` is given for you to apply association mining and sequence mining. \n",
    "\n",
    "## 1. Data Mining <a name=\"intro\"></a>\n",
    "After the data is cleaned, models can be built to perform in-depth analysis. There are two broad categories of data mining: **Predictive Mining** (e.g. classification and regression with decision tree, regression and neural network) and **Descriptive Mining** (e.g. clustering and association mining). Many algorithms belonging to each of the categories are available in `sklearn`, each with its own characteristics. The upcoming practical notes will explore some of these algorithms in detail.\n",
    "\n",
    "Data mining outcomes are best understood when accompanied with graphs and charts of patterns and trends identified in the data. Visualisation allows us to understand the data better. In this unit, all visualisations will be done using `seaborn` and `matplotlib` with data presented by `pandas` DataFrames.\n",
    "\n",
    "## 2. Performing Association Mining <a name=\"am\"></a>\n",
    "\n",
    "In these notes, we will focus on how to pre-process the dataset called 'bank.csv' and perform association rule mining and sequence pattern mining. \n",
    "\n",
    "A bank’s Marketing department is interested in examining associations between various retail banking services used by its customers. The Marketing department would like to determine both typical and atypical service combinations. The dataset is provided in the file `bank.csv`.\n",
    "\n",
    "These requirements will suffice to conducting association mining on the dataset, a market basket analysis. The\n",
    "data for this problem consists of two variables: a transaction ID and an item. For each transaction, there is a list of items. For the banking\n",
    "dataset, a transaction is an individual customer account, and items are products bought by the customer. An association rule is a statement of the form (item set A) => (item set B).\n",
    "\n",
    "Recall from the lecture that the most common association rule mining algorith is **Apriori algorithm**. Unfortunately, `sklearn` does not provide an implementation of Apriori algorithm. Therefore, we will install and use a\n",
    "library called `apyori` for this task.\n",
    "\n",
    "Go to Anaconda prompt and install the library using this command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install apyori\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the library is installed, we need to perform some data preprocessing on the `bank` dataset. Firstly, load the data set using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32367 entries, 0 to 32366\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   ACCOUNT  32367 non-null  int64 \n",
      " 1   SERVICE  32367 non-null  object\n",
      " 2   VISIT    32367 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 758.7+ KB\n",
      "None\n",
      "   ACCOUNT SERVICE  VISIT\n",
      "0   500026   CKING      1\n",
      "1   500026     SVG      2\n",
      "2   500026     ATM      3\n",
      "3   500026     ATM      4\n",
      "4   500075   CKING      1\n",
      "5   500075    MMDA      2\n",
      "6   500075     SVG      3\n",
      "7   500075     ATM      4\n",
      "8   500075   TRUST      5\n",
      "9   500075   TRUST      6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the bank transaction dataset\n",
    "df = pd.read_csv('bank.csv')\n",
    "\n",
    "# info and the first 10 transactions\n",
    "print(df.info())\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BANK data set contains service information of nearly 8,000 customers. There are three variables in the data set:\n",
    "1. ACCOUNT: Account number, nominal\n",
    "2. SERVICE: Type of service, nominal\n",
    "3. VISIT: Order of product purchased, ordinal\n",
    "\n",
    "The dataset has over 32,000 rows. Each row represents a customer-service combination. Therefore, a single customer can have multiple rows in the data set, and each row represents one of the products he or she owns. The median number of products per customer is three. The 13 products are represented in the data set using the following abbreviations:\n",
    "\n",
    "* ATM - automated teller machine debit card\n",
    "* AUTO automobile installment loan\n",
    "* CCRD credit card\n",
    "* CD certificate of deposit\n",
    "* CKCRD check/debit card\n",
    "* CKING checking account\n",
    "* HMEQLC home equity line of credit\n",
    "* IRA individual retirement account\n",
    "* MMDA money market deposit account\n",
    "* MTG mortgage\n",
    "* PLOAN personal/consumer installment loan\n",
    "* SVG saving account\n",
    "* TRUST personal trust account\n",
    "\n",
    "As we are looking to generate association rules from items purchased by each account holder, we need to group the accounts and then generate the list of all services purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCOUNT\n",
      "500026                   [CKING, SVG, ATM, ATM]\n",
      "500075    [CKING, MMDA, SVG, ATM, TRUST, TRUST]\n",
      "500129              [CKING, SVG, IRA, ATM, ATM]\n",
      "500256               [CKING, SVG, CKCRD, CKCRD]\n",
      "500341               [CKING, SVG, CKCRD, CKCRD]\n",
      "Name: SERVICE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# group by account, then list all services\n",
    "transactions = df.groupby(['ACCOUNT'])['SERVICE'].apply(list)\n",
    "\n",
    "print(transactions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `transactions` table containing all services purchased by each account number is populated, we are ready to generate association rules. The `apyori`'s `apriori` function accepts a number of parameters, mainly:\n",
    "1. `transactions`: list of list of items in transactions (eg. [['A', 'B'], ['B', 'C']]).\n",
    "2. `min_support`: Minimum support of relations in float percentage. It specifies a minimum level of support to claim that items are associated (i.e. they occur together in the dataset). Default 0.1.\n",
    "3. `min_confidence`: Minimum confidence of relations in float percentage. Default 0.0.\n",
    "4. `min_lift`: Minimum lift of relations in float percentage. Default 0.0.\n",
    "5. `max_length`: Max length of the relations. Default None.\n",
    "\n",
    "Note: Parameters `min_support` and `min_confidence` control the numbers and types of rules generated. There are many heuristics that you can apply to set these numbers. (1) If you are interested in generating associations that involve fairly rare products, you should consider reducing `min_support`. (2) If the items present in the dataset do not show high support, 'min_support' threshold shoudl be set to small value and vice-versa. (3) If you obtain too many rules to be practically useful, you should consider increasing `min_suport` and `min_confidence` as a possible solution\n",
    "\n",
    "We will run the `apyori` model with the pre-processed transactions and `min_support` of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RelationRecord(items=frozenset({'ATM'}), support=0.3845576273307471, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'ATM'}), confidence=0.3845576273307471, lift=1.0)]), RelationRecord(items=frozenset({'AUTO'}), support=0.09285446126892755, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'AUTO'}), confidence=0.09285446126892755, lift=1.0)]), RelationRecord(items=frozenset({'CCRD'}), support=0.154799149042673, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'CCRD'}), confidence=0.154799149042673, lift=1.0)]), RelationRecord(items=frozenset({'CD'}), support=0.24527593542735576, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'CD'}), confidence=0.24527593542735576, lift=1.0)]), RelationRecord(items=frozenset({'CKCRD'}), support=0.11300212739331748, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'CKCRD'}), confidence=0.11300212739331748, lift=1.0)])]\n"
     ]
    }
   ],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "# type cast the transactions from pandas into normal list format and run apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support=0.05))\n",
    "\n",
    "# print first 5 rules\n",
    "print(results[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output may look very cluttered. The following function can be used to printing them neatly. We will not go deeper to explain how it works and it is not essential for your learning objective, but we have included some comments to help you out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Left_side  Right_side   Support  Confidence      Lift\n",
      "0                    ATM  0.384558    0.384558  1.000000\n",
      "1                   AUTO  0.092854    0.092854  1.000000\n",
      "2                   CCRD  0.154799    0.154799  1.000000\n",
      "3                     CD  0.245276    0.245276  1.000000\n",
      "4                  CKCRD  0.113002    0.113002  1.000000\n",
      "5                  CKING  0.857840    0.857840  1.000000\n",
      "6                 HMEQLC  0.164685    0.164685  1.000000\n",
      "7                    IRA  0.108372    0.108372  1.000000\n",
      "8                   MMDA  0.174446    0.174446  1.000000\n",
      "9                    MTG  0.074334    0.074334  1.000000\n",
      "10                   SVG  0.618696    0.618696  1.000000\n",
      "11                CD,ATM  0.071581    0.071581  1.000000\n",
      "12       ATM          CD  0.071581    0.186137  0.758889\n",
      "13        CD         ATM  0.071581    0.291837  0.758889\n",
      "14             CKING,ATM  0.361907    0.361907  1.000000\n",
      "15       ATM       CKING  0.361907    0.941100  1.097058\n",
      "16     CKING         ATM  0.361907    0.421882  1.097058\n",
      "17            HMEQLC,ATM  0.085346    0.085346  1.000000\n",
      "18       ATM      HMEQLC  0.085346    0.221933  1.347619\n",
      "19    HMEQLC         ATM  0.085346    0.518237  1.347619\n"
     ]
    }
   ],
   "source": [
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "    \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', \n",
    "                                        'Confidence', 'Lift']) \n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table contains statistics of support, confidence and lift for each of the rules.\n",
    "\n",
    "Consider the rule A ⇒ B. Recall the following theoretical concepts:\n",
    "* Support of A ⇒ B is the probability that a customer has both A and B.\n",
    "* Confidence of A ⇒ B is the probability that a customer has B given that the customer has A.\n",
    "* Lift of A ⇒ B is a measure of strength of the association. The Lift=2 for the rule A=>B indicates that a customer having A is twice as likely to have B than a customer chosen at random.\n",
    "\n",
    "In a typical setting, we would like to view the rules by lift value. Sort the rules using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Left_side     Right_side   Support  Confidence      Lift\n",
      "131          CKCRD     CCRD,CKING  0.055813    0.493909  3.325045\n",
      "134     CCRD,CKING          CKCRD  0.055813    0.375737  3.325045\n",
      "33            CCRD          CKCRD  0.055813    0.360550  3.190645\n",
      "130           CCRD    CKING,CKCRD  0.055813    0.360550  3.190645\n",
      "135    CKING,CKCRD           CCRD  0.055813    0.493909  3.190645\n",
      "34           CKCRD           CCRD  0.055813    0.493909  3.190645\n",
      "203     HMEQLC,SVG      CKING,ATM  0.060944    0.546577  1.510268\n",
      "198      CKING,ATM     HMEQLC,SVG  0.060944    0.168396  1.510268\n",
      "196         HMEQLC  CKING,SVG,ATM  0.060944    0.370061  1.489001\n",
      "205  CKING,SVG,ATM         HMEQLC  0.060944    0.245217  1.489001\n"
     ]
    }
   ],
   "source": [
    "# sort all acquired rules descending by lift\n",
    "result_df = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(result_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest lift rule is *checking*, and *credit card* implies *check card*. This is not surprising given that many check cards include credit card logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set the `min_support` to 0.01 and list the rules generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Left_side     Right_side   Support  Confidence      Lift\n",
      "131          CKCRD     CCRD,CKING  0.055813    0.493909  3.325045\n",
      "134     CCRD,CKING          CKCRD  0.055813    0.375737  3.325045\n",
      "33            CCRD          CKCRD  0.055813    0.360550  3.190645\n",
      "130           CCRD    CKING,CKCRD  0.055813    0.360550  3.190645\n",
      "135    CKING,CKCRD           CCRD  0.055813    0.493909  3.190645\n",
      "34           CKCRD           CCRD  0.055813    0.493909  3.190645\n",
      "203     HMEQLC,SVG      CKING,ATM  0.060944    0.546577  1.510268\n",
      "198      CKING,ATM     HMEQLC,SVG  0.060944    0.168396  1.510268\n",
      "196         HMEQLC  CKING,SVG,ATM  0.060944    0.370061  1.489001\n",
      "205  CKING,SVG,ATM         HMEQLC  0.060944    0.245217  1.489001\n"
     ]
    }
   ],
   "source": [
    "results2 = list(apriori(transaction_list, min_support=0.05))\n",
    "result2_df = convert_apriori_results_to_pandas_df(results2)\n",
    "# sort all acquired rules descending by lift\n",
    "result2_df = result2_df.sort_values(by='Lift', ascending=False)\n",
    "print(result2_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performing Sequential Rule Mining Using SPMF <a name=\"sm\"></a>\n",
    "\n",
    "This section introduces you how to use an algorithm implemnetd in another lanaguge. There exists no mature library in Python that implements sequential rule mining. On the other hand, SPMF is a Java library implemented by Philippe Fournier Viger containing 150 pattern mining algorithms, such as frequent pattern mining, association rules, frequent sequences and of course, sequential rule mining. We will use SPMF to perform sequential rule mining on this dataset.\n",
    "\n",
    "More information about SPMF can be found here http://www.philippe-fournier-viger.com/spmf/index.php and you can download SPMF here http://www.philippe-fournier-viger.com/spmf/index.php?link=download.php. Put the `spmf.jar` file into the same directory with this Jupyter notebook.\n",
    "\n",
    "As SPMF is implemented in Java, you need to install Java to run the `.jar` file. There exist many tutorials on the Internet. To check if you have installed Java correctly for this practical, open your terminal/CMD and type `java`.\n",
    "\n",
    "To demonstrate how to use SPMF through Python, we will use the same `bank.csv` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from association mining, to get sequential rules, there must be information on the order of the target. In this dataset, the order is given in `VISIT` column. As this dataset is already ordered by `VISIT`, a simple group and apply list will produce sequences in order. In other datasets, you may have to perform some preprocessing to ensure this ordering is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['CKING', 'SVG', 'ATM', 'ATM'], ['CKING', 'MMDA', 'SVG', 'ATM', 'TRUST', 'TRUST'], ['CKING', 'SVG', 'IRA', 'ATM', 'ATM'], ['CKING', 'SVG', 'CKCRD', 'CKCRD'], ['CKING', 'SVG', 'CKCRD', 'CKCRD']]\n"
     ]
    }
   ],
   "source": [
    "transactions = df.groupby(['ACCOUNT'])['SERVICE'].apply(list)\n",
    "sequences = transactions.values.tolist()\n",
    "\n",
    "# show the first 5 sequences\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have sequences ordered correctly, you could simply run the following function to get sequential rules and their respective support and confidence. In general, the function below will write the sequences into a file called `seq_rule_input.txt` in SPMF accepted format, run SPMF to generate sequential rules, read the output file and return a Pandas dataframe. Detailed comments is provided in the function source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "''' Uses SPMF to find association rules in supplied transactions '''\n",
    "def get_association_rules(sequences, min_sup, min_conf):\n",
    "    # step 1: create required input for SPMF\n",
    "    \n",
    "    # prepare a dict to uniquely assign each item in the transactions to an int ID\n",
    "    item_dict = defaultdict(int)\n",
    "    output_dict = defaultdict(str)\n",
    "    item_id = 1\n",
    "    \n",
    "    # write your sequences in SPMF format\n",
    "    with open('seq_rule_input.txt', 'w+') as f:\n",
    "        for sequence in sequences:\n",
    "            z = []\n",
    "            for itemset in sequence:\n",
    "                # if there are multiple items in one itemset\n",
    "                if isinstance(itemset, list):\n",
    "                    for item in itemset:\n",
    "                        if item not in item_dict:\n",
    "                            item_dict[item] = item_id\n",
    "                            item_id += 1\n",
    "\n",
    "                        z.append(item_dict[item])\n",
    "                else:\n",
    "                    if itemset not in item_dict:\n",
    "                        item_dict[itemset] = item_id\n",
    "                        output_dict[str(item_id)] = itemset\n",
    "                        item_id += 1\n",
    "                    z.append(item_dict[itemset])\n",
    "                    \n",
    "                # end of itemset\n",
    "                z.append(-1)\n",
    "            \n",
    "            # end of a sequence\n",
    "            z.append(-2)\n",
    "            f.write(' '.join([str(x) for x in z]))\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # run SPMF with supplied parameters\n",
    "    supp_param = '{}%'.format(int(min_sup * 100))\n",
    "    conf_param = '{}%'.format(int(min_conf * 100))\n",
    "    subprocess.call(['java', '-jar', 'spmf.jar', 'run', 'RuleGrowth', \n",
    "                     'seq_rule_input.txt', 'seq_rule_output.txt', \n",
    "                     supp_param, conf_param], shell=True)\n",
    "    \n",
    "    # read back the output rules\n",
    "    outputs = open('seq_rule_output.txt', 'r').read().strip().split('\\n')\n",
    "    output_rules = []\n",
    "    for rule in outputs:\n",
    "        left, right, sup, conf = re.search(pattern=r'([0-9\\,]+) ==> ([0-9\\,]+) #SUP: ([0-9]+) #CONF: ([0-9\\.]+)', string=rule).groups()\n",
    "        sup = int(sup) / len(sequences)\n",
    "        conf = float(conf)\n",
    "        output_rules.append([[output_dict[x] for x in left.split(',')], [output_dict[x] for x in right.split(',')], sup, conf])\n",
    "    \n",
    "    # return pandas DataFrame\n",
    "    return pd.DataFrame(output_rules, columns = ['Left_rule', 'Right_rule', 'Support', 'Confidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the function on sequences from the `bank.csv` dataset using the command below. In here, we are using `min_supp` of 0.1 and `min_conf` of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left_rule</th>\n",
       "      <th>Right_rule</th>\n",
       "      <th>Support</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[SVG]</td>\n",
       "      <td>0.541734</td>\n",
       "      <td>0.631510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[SVG, ATM]</td>\n",
       "      <td>0.248530</td>\n",
       "      <td>0.289716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[SVG, CD]</td>\n",
       "      <td>0.142535</td>\n",
       "      <td>0.166156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[SVG, HMEQLC]</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.129978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[ATM]</td>\n",
       "      <td>0.361907</td>\n",
       "      <td>0.421882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[CKING, SVG]</td>\n",
       "      <td>[ATM]</td>\n",
       "      <td>0.248530</td>\n",
       "      <td>0.458766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[MMDA]</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.181619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[CKCRD]</td>\n",
       "      <td>0.113002</td>\n",
       "      <td>0.131729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[CD]</td>\n",
       "      <td>0.209861</td>\n",
       "      <td>0.244639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[CKING, SVG]</td>\n",
       "      <td>[CD]</td>\n",
       "      <td>0.142535</td>\n",
       "      <td>0.263109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[CCRD]</td>\n",
       "      <td>0.148542</td>\n",
       "      <td>0.173158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[CKING]</td>\n",
       "      <td>[HMEQLC]</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>0.191977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[CKING, SVG]</td>\n",
       "      <td>[HMEQLC]</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.205821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[SVG]</td>\n",
       "      <td>[ATM]</td>\n",
       "      <td>0.256914</td>\n",
       "      <td>0.415251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[SVG]</td>\n",
       "      <td>[CD]</td>\n",
       "      <td>0.157177</td>\n",
       "      <td>0.254045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[SVG]</td>\n",
       "      <td>[CCRD]</td>\n",
       "      <td>0.102240</td>\n",
       "      <td>0.165251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[SVG]</td>\n",
       "      <td>[HMEQLC]</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.180218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Left_rule     Right_rule   Support  Confidence\n",
       "0        [CKING]          [SVG]  0.541734    0.631510\n",
       "1        [CKING]     [SVG, ATM]  0.248530    0.289716\n",
       "2        [CKING]      [SVG, CD]  0.142535    0.166156\n",
       "3        [CKING]  [SVG, HMEQLC]  0.111500    0.129978\n",
       "4        [CKING]          [ATM]  0.361907    0.421882\n",
       "5   [CKING, SVG]          [ATM]  0.248530    0.458766\n",
       "6        [CKING]         [MMDA]  0.155800    0.181619\n",
       "7        [CKING]        [CKCRD]  0.113002    0.131729\n",
       "8        [CKING]           [CD]  0.209861    0.244639\n",
       "9   [CKING, SVG]           [CD]  0.142535    0.263109\n",
       "10       [CKING]         [CCRD]  0.148542    0.173158\n",
       "11       [CKING]       [HMEQLC]  0.164685    0.191977\n",
       "12  [CKING, SVG]       [HMEQLC]  0.111500    0.205821\n",
       "13         [SVG]          [ATM]  0.256914    0.415251\n",
       "14         [SVG]           [CD]  0.157177    0.254045\n",
       "15         [SVG]         [CCRD]  0.102240    0.165251\n",
       "16         [SVG]       [HMEQLC]  0.111500    0.180218"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_association_rules(sequences, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first rule is `CKING` => `SVG` with 0.54 support and 0.63 confidence. This is a strong rule. The support value implies that 54% of customers bought `SVG` after `CKING`. The confidence value implies that if a customer has bought `CKING`, the probability of them buying `SVG` subsequently is 63%.\n",
    "\n",
    "## End Notes\n",
    "\n",
    "We learned how to build, tune and explore association mining models. We also used visualisation to help us explain the association rules. The goal of association mining is to identify association among varaiables without the presence of target variable explictly and training of the model. This analysis is based on frequency of items present in the transactional dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
